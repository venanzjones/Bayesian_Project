- Su Na totali
Analisi introduttiva zero è stata vedere dove sono i dati mancanti, e capire come affrontarli, i plot fanno vedere che i giorni mancati o i dati mancanti sono distribuiti in modo pseudo casuale e quindi di difficile interpretazione, abbiamo deciso quindi di accettare una soglia del 10% per i valori mancanti nel dataset finale, e abbiamo deciso dei thresholds per accettare o rifiutare un mese.
Worth noticing: queste sono le medie di Na divise per mese, forse dipendono anche dalle vacanze estive: 49.00000 49.33333 77.50980 82.98039 98.62745 72.66667 62.76471 60.33333.
- Su sistemazione datasets e riempimento valori mancanti:
L'idea di usare la media o interpolazione che sia per riempire i giorni mancanti a livello inferiore rispetto al dataset che useremo nel modello vero e proprio, è data dal fatto che per stimarle con un pacchetto (mice) sarebbero dovute entrare in gioco altre covariate, che a questo stage del progetto non sono ancora state interamente (neanche in parte in realtà) definite. Andare quindi ad ingrossare ancora il modello porterebbe una complessità eccessiva sempre a questo stage, e soprattutto la ricerca di covariate giornaliere che non sono il focus del nostro problema, questo mismatch convince ancora di più nello stimare tramite interpolazione, sarebbe un problema altrettanto complicato cercare queste nuove covariate.
Essendo handling le dimensioni da affrontare per fare le medie tra massimi giornalieri, allora ci affidiamo a questo algoritmo per stimare (sempre e solo se ce ne sono poche) le quantità mancanti mensili.
Questo ci porta alla definizione di Na, che avendo tre livelli di complessità non è immediato e di come e a quale di questi livelli studiare la media.
. Dataset 180
Dai primi insight iniziali su tutto il dataset, una soglia ritenuta buona come dati da simulare è stata pensata a 5: se ho 5 o meno giorni mancanti nel mese, quesi giorni verranno simulati dal nostro algoritmo. Adesso però il problema è come simulare questi giorni e come valutare i dati mancanti a livello di ore, che poi è il nostro starting point nel progetto.
A questi propositi, la seconda analisi fatta è stata sulle misurazioni orarie, quindi un giorno viene inizialmente considerato valido, ovvero non porta informazione con se (se non ha un'ora che supera i 180, in quel caso il giorno informa anche se ha solo un valore con se) se gli mancano 8 o più ore(questa soglia verrà poi modificata).
Il dato che noi andiamo a estrarre dal dataset è legato alla variabile aleatoria del massimo delle misurazioni giornaliere, e consideriamo quante di queste misure superano i 180. A questo punto, la strategia considerata per simulare i dati è stata un interpolazione lineare tra il massimo osservato nel giorno più vicino prima e dopo la sequenza di giorni considerati non validi. A questo punto un dataset completo è stato valutato in giorni con distribuzione maggiore di 180 e il dataset finale è quello che sta sotto a Dataset_180.
Da un plot degli Na si vede come alcune stazioni abbiano una soglia di Na enorma rispetto alle altre, e che quindi andranno per forza rimosse dall'analisi in quanto non si riesce a fare inferenza se abbiamo un numero intorno al 30% di NA per quella stazione negli effetti misti. Soglie da rivedere insieme in quest'ottica, infatti una volta rimosse alcune stazioni il codice va rirunnato e i threshold modificati di conseguenza (?) no perchè altrimenti entriamo in un loop infinito.
.Dataset_120
Qua il livello di complessità si alza, le medie orarie complicano la situazione aggiungendo un livello di complessità in più. Innanzitutto il dataset creato, è quello con le medie orarie che poi verrà utilizzato per costruire i count. 
Per creare il dataset con le medie orarie abbiamo dovuto confrontarci con i valori mancanti, infatti quasi ogni giorno nella nostra rilevazione presentano diversi valori mancanti. La scelta è ricaduta sul considerare la media tra valori valida se abbiamo più di 4 valori effettivi misurati. Una volta creato questo nuovo dataset di valori che sono medie mobili, il processo per selezionare i giorni che danno un conto sopra soglia lo abbiamo scelto analogo a quello usato per il Dataset_180. Le soglie sono specifiche per il problema in quanto abbiamo spinto verso il basso il più possibile per ottenere il massimo a livello di informazione possibile.
Vedere i threshold precisi, e infine il discorso sulle stazioni specifiche che vanno trascurate vale equamente.
