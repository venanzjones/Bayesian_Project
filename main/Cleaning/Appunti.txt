- Su sistemazione datasets e riempimento valori mancanti:
L'idea di usare la media o interpolazione che sia per riempire i giorni mancanti a livello inferiore rispetto al dataset che useremo nel modello vero e proprio, è data dal fatto che per stimarle con un pacchetto (mice) sarebbero dovute entrare in gioco altre covariate, che a questo stage del progetto non sono ancora state interamente (neanche in parte in realtà) definite. Andare quindi ad ingrossare ancora il modello porterebbe una complessità eccessiva sempre a questo stage, e soprattutto la ricerca di covariate giornaliere che non sono il focus del nostro problema, questo mismatch convince ancora di più nello stimare tramite interpolazione, sarebbe un problema altrettanto complicato cercare queste nuove covariate.
Essendo handling le dimensioni da affrontare per fare le medie tra massimi giornalieri, allora ci affidiamo a questo algoritmo per stimare (sempre e solo se ce ne sono poche) le quantità mancanti mensili.
Questo ci porta alla definizione di Na, che avendo tre livelli di complessità non è immediato e di come e a quale di questi livelli studiare la media.
. Dataset 180
Dai primi insight iniziali su tutto il dataset, una soglia ritenuta buona come dati da simulare è stata pensata a 5: se ho 5 o meno giorni mancanti nel mese, quesi giorni verranno simulati dal nostro algoritmo. Adesso però il problema è come simulare questi giorni e come valutare i dati mancanti a livello di ore, che poi è il nostro starting point nel progetto.
A questi propositi, la seconda analisi fatta è stata sulle misurazioni orarie, quindi un giorno viene inizialmente considerato valido, ovvero non porta informazione con se (se non ha un'ora che supera i 180, in quel caso il giorno informa anche se ha solo un valore con se) se gli mancano 8 o più ore(questa soglia verrà poi modificata).
Il dato che noi andiamo a estrarre dal dataset è legato alla variabile aleatoria del massimo delle misurazioni giornaliere, e consideriamo quante di queste misure superano i 180. A questo punto, la strategia considerata per simulare i dati è stata un interpolazione lineare tra il massimo osservato nel giorno più vicino prima e dopo la sequenza di giorni considerati non validi. A questo punto un dataset completo è stato valutato in giorni con distribuzione maggiore di 180 e il dataset finale è quello che sta sotto a Dataset_180.
Da un plot degli Na si vede come alcune stazioni abbiano una soglia di Na enorma rispetto alle altre, e che quindi andranno per forza rimosse dall'analisi in quanto non si riesce a fare inferenza se abbiamo un numero intorno al 30% di NA per quella stazione negli effetti misti. Soglie da rivedere insieme in quest'ottica, infatti una volta rimosse alcune stazioni il codice va rirunnato e i threshold modificati di conseguenza (?) no perchè altrimenti entriamo in un loop infinito.

